{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import mne\n",
    "\n",
    "from gait_modulation.file_reader import MatFileReader\n",
    "from gait_modulation.data_processor import DataProcessor\n",
    "from gait_modulation.viz import Visualise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/EM_FH_HK/PW_HK59/26_10_22/walking_sync_2_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/EM_FH_HK/PW_HK59/26_10_22/walking_sync_3_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/EM_FH_HK/PW_FH57/no_date/walking_sync_4_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/EM_FH_HK/PW_FH57/no_date/walking_sync_2_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/EM_FH_HK/PW_FH57/no_date/walking_sync_3_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/EM_FH_HK/PW_EM59/21_07_2023/walking_sync_4_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/EM_FH_HK/PW_EM59/21_07_2023/walking_sync_3_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/HZ_SN_SN_US/PW_HZ58/15_09_23/walking_sync_6_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/HZ_SN_SN_US/PW_HZ58/15_09_23/walking_sync_4_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/HZ_SN_SN_US/PW_HZ58/15_09_23/walking_sync_3_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/HZ_SN_SN_US/PW_US68/15_02_23/walking_sync_1_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/HZ_SN_SN_US/PW_SN66/20_09_23/walking_sync_1_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/HZ_SN_SN_US/PW_SN61/20_03_23/walking_sync_1_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/HZ_SN_SN_US/PW_SN61/20_03_23/walking_sync_4_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/HZ_SN_SN_US/PW_SN61/20_03_23/walking_sync_2_short.mat\n",
      "Loading data from file: /Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data/HZ_SN_SN_US/PW_SN61/20_03_23/walking_sync_3_short.mat\n",
      "Number of sessions: 16\n"
     ]
    }
   ],
   "source": [
    "# Handle multiple patients with nested directories.\n",
    "root_directory = '/Users/orabe/Library/Mobile Documents/com~apple~CloudDocs/0_TU/Master/master_thesis/Chiara/organized_data'\n",
    "mat_reader = MatFileReader(root_directory, max_workers=1)  #  adjust the number of workers for parallelism\n",
    "\n",
    "# Read all data from nested folders of multiple patients and sessions\n",
    "all_data = mat_reader.read_data()\n",
    "n_sessions = len(all_data)\n",
    "\n",
    "print(f\"Number of sessions: {n_sessions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session: 0\n",
      "Number of samples removed: 14738\n",
      "Number of seconds removed: 58.95 seconds\n",
      "Total epochs: 105\n",
      "mod_start: 8 epochs; normal_walking: 97 epochs; Using matplotlib as 2D backend.\n",
      "\n",
      "==========================================================\n",
      "Session: 1\n",
      "No trimming needed as the beginning of signal is not flat.\n",
      "Total epochs: 64\n",
      "mod_start: 6 epochs; normal_walking: 58 epochs; \n",
      "==========================================================\n",
      "Session: 2\n",
      "Number of samples removed: 2282\n",
      "Number of seconds removed: 9.13 seconds\n",
      "Total epochs: 25\n",
      "mod_start: 15 epochs; normal_walking: 10 epochs; \n",
      "==========================================================\n",
      "Session: 3\n",
      "Number of samples removed: 7049\n",
      "Number of seconds removed: 28.20 seconds\n",
      "Total epochs: 21\n",
      "mod_start: 12 epochs; normal_walking: 9 epochs; \n",
      "==========================================================\n",
      "Session: 4\n",
      "Number of samples removed: 1950\n",
      "Number of seconds removed: 7.80 seconds\n",
      "Total epochs: 53\n",
      "mod_start: 8 epochs; normal_walking: 45 epochs; "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/skjgj9ps5xs02bklf7l0nsjr0000gn/T/ipykernel_36988/1050252588.py:109: RuntimeWarning: Omitted 7 annotation(s) that were outside data range.\n",
      "  lfp_raw.set_annotations(my_annot)\n",
      "/var/folders/jk/skjgj9ps5xs02bklf7l0nsjr0000gn/T/ipykernel_36988/1050252588.py:109: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  lfp_raw.set_annotations(my_annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "Session: 5\n",
      "No trimming needed as the beginning of signal is not flat.\n",
      "Total epochs: 28\n",
      "mod_start: 11 epochs; normal_walking: 17 epochs; "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/skjgj9ps5xs02bklf7l0nsjr0000gn/T/ipykernel_36988/1050252588.py:109: RuntimeWarning: Omitted 1 annotation(s) that were outside data range.\n",
      "  lfp_raw.set_annotations(my_annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "Session: 6\n",
      "Number of samples removed: 1819\n",
      "Number of seconds removed: 7.28 seconds\n",
      "Total epochs: 40\n",
      "mod_start: 8 epochs; normal_walking: 32 epochs; \n",
      "==========================================================\n",
      "Session: 7\n",
      "Number of samples removed: 672\n",
      "Number of seconds removed: 2.69 seconds\n",
      "Total epochs: 26\n",
      "mod_start: 11 epochs; normal_walking: 15 epochs; \n",
      "==========================================================\n",
      "Session: 8\n",
      "No trimming needed as the beginning of signal is not flat.\n",
      "Total epochs: 40\n",
      "mod_start: 13 epochs; normal_walking: 27 epochs; \n",
      "==========================================================\n",
      "Session: 9\n",
      "Number of samples removed: 731\n",
      "Number of seconds removed: 2.92 seconds\n",
      "Total epochs: 26\n",
      "mod_start: 15 epochs; normal_walking: 11 epochs; \n",
      "==========================================================\n",
      "Session: 10\n",
      "No trimming needed as the beginning of signal is not flat.\n",
      "Total epochs: 85\n",
      "mod_start: 3 epochs; normal_walking: 82 epochs; \n",
      "==========================================================\n",
      "Session: 11\n",
      "Number of samples removed: 3215\n",
      "Number of seconds removed: 12.86 seconds\n",
      "Total epochs: 54\n",
      "mod_start: 10 epochs; normal_walking: 44 epochs; "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/skjgj9ps5xs02bklf7l0nsjr0000gn/T/ipykernel_36988/1050252588.py:109: RuntimeWarning: Omitted 2 annotation(s) that were outside data range.\n",
      "  lfp_raw.set_annotations(my_annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "Session: 12\n",
      "Number of samples removed: 6608\n",
      "Number of seconds removed: 26.43 seconds\n",
      "Total epochs: 66\n",
      "mod_start: 5 epochs; normal_walking: 61 epochs; \n",
      "==========================================================\n",
      "Session: 13\n",
      "Number of samples removed: 13713\n",
      "Number of seconds removed: 54.85 seconds\n",
      "Total epochs: 48\n",
      "mod_start: 5 epochs; normal_walking: 43 epochs; "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/skjgj9ps5xs02bklf7l0nsjr0000gn/T/ipykernel_36988/1050252588.py:109: RuntimeWarning: Omitted 2 annotation(s) that were outside data range.\n",
      "  lfp_raw.set_annotations(my_annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "Session: 14\n",
      "Number of samples removed: 1090\n",
      "Number of seconds removed: 4.36 seconds\n",
      "Total epochs: 42\n",
      "mod_start: 9 epochs; normal_walking: 33 epochs; \n",
      "==========================================================\n",
      "Session: 15\n",
      "Number of samples removed: 13850\n",
      "Number of seconds removed: 55.40 seconds\n",
      "Total epochs: 34\n",
      "mod_start: 8 epochs; normal_walking: 26 epochs; \n",
      "==========================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        \n",
       "        <td>mod_start: 147<br/>normal_walking: 610</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>-2.000 – 0.000 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>off</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<EpochsArray |  757 events (all good), -2 – 0 s, baseline off, ~17.4 MB, data loaded,\n",
       " 'mod_start': 147\n",
       " 'normal_walking': 610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access specific sessions for a patient\n",
    "session = all_data[0] # pick any session e.g. first one to load the meta data\n",
    "\n",
    "# Extract LFP meta data for subject/session\n",
    "lfp_metadata = DataProcessor.np_to_dict(session['hdr_LFP'])\n",
    "\n",
    "# Load LFP parameters\n",
    "lfp_sfreq = lfp_metadata['Fs'].item()\n",
    "lfp_ch_names = DataProcessor.rename_lfp_channels(lfp_metadata['labels'])\n",
    "lfp_n_channels = lfp_metadata['NumberOfChannels'].item()\n",
    "\n",
    "# Prepare for mne data structure\n",
    "info = mne.create_info(ch_names=lfp_ch_names[0:6], sfreq=lfp_sfreq, ch_types='dbs', verbose=40)\n",
    "\n",
    "# Select one event to work with: mod_start\n",
    "event_of_interest = 'mod_start'\n",
    "mod_start_event_id = 1\n",
    "\n",
    "# Define normal walking events\n",
    "normal_walking_event_id = -1\n",
    "\n",
    "# Define the event dictionary\n",
    "event_dict = {\n",
    "    'mod_start': mod_start_event_id,\n",
    "    'normal_walking': normal_walking_event_id\n",
    "}\n",
    "\n",
    "# Define parameters\n",
    "epoch_tmin = -2.0\n",
    "epoch_tmax = 0.0\n",
    "epoch_duration = epoch_tmax - epoch_tmin\n",
    "epoch_sample_length = int(epoch_duration * lfp_sfreq)\n",
    "gap_duration = 10  # At least 10 seconds away from modulation events\n",
    "gap_sample_length = int(gap_duration * lfp_sfreq)\n",
    "\n",
    "epochs_list = []\n",
    "events_list = []\n",
    "\n",
    "for s in range(n_sessions):\n",
    "    print(f'Session: {s}')\n",
    "    session = all_data[s] # Access specific patient/sessions\n",
    "\n",
    "    # Extract events and lfp data of the subject/session\n",
    "    lfp_data = session['data_LFP'] * 1e-6  # Convert microvolts to volts\n",
    "    \n",
    "    # lfp_raw = mne.io.RawArray(lfp_data, info, verbose=40)\n",
    "    # lfp_raw.plot(start=0, duration=np.inf, remove_dc=False)\n",
    "    # plt.show()\n",
    "\n",
    "    # Handle events\n",
    "    events_KIN = DataProcessor.np_to_dict(session['events_KIN'])\n",
    "    events_before_trim, event_dict_before_trim = DataProcessor.create_events_array(events_KIN, lfp_sfreq)\n",
    "\n",
    "    # Trim the data and adjust the event onsets accordingly\n",
    "    lfp_data, events_after_trim = DataProcessor.trim_data(lfp_data, events_before_trim, lfp_sfreq)\n",
    "    lfp_duration = lfp_data.shape[1] / lfp_sfreq\n",
    "    n_samples = int(lfp_duration * lfp_sfreq)\n",
    "\n",
    "    # Update raw data after trimming\n",
    "    lfp_raw = mne.io.RawArray(lfp_data, info, verbose=40)\n",
    "\n",
    "    # events_mod_start = events_before_trim[events_before_trim[:, 2] == event_dict_before_trim[event_of_interest]]\n",
    "    events_mod_start = events_after_trim[events_after_trim[:, 2] == event_dict_before_trim[event_of_interest]]\n",
    "    events_mod_start[:, 1] = s # mark the session nr  \n",
    "    # print(\"--->\", np.unique(events_mod_start[:, 1]), np.unique(events_mod_start[:, 2], return_counts=True))\n",
    "\n",
    "    # Rename Gait Modulation Events\n",
    "    events_mod_start[:, 2] = mod_start_event_id\n",
    "        \n",
    "    # Define normal walking events\n",
    "    normal_walking_events = DataProcessor.define_normal_walking_events(\n",
    "        normal_walking_event_id, events_mod_start,\n",
    "        gap_sample_length, epoch_sample_length, n_samples\n",
    "    )\n",
    "    \n",
    "    events_mod_start[:, 1] = s # mark the session nr\n",
    "    normal_walking_events[:, 1] = s # mark the session nr\n",
    "\n",
    "    # ## Remove artifacts from raw LFP data using ICA.\n",
    "    # ica_n_components = 6 # 6 = n_channels.\n",
    "    # ica = mne.preprocessing.ICA(n_components=ica_n_components, random_state=97, max_iter=800, verbose=40)\n",
    "    # print(lfp_raw.ch_names)\n",
    "    # ica.fit(lfp_raw)\n",
    "    # raw_data_clean = ica.apply(lfp_raw, verbose=40) # Apply ICA to the raw data\n",
    "\n",
    "    # Combine events and create epochs\n",
    "    events, epochs = DataProcessor.create_epochs_with_events(\n",
    "        lfp_raw,\n",
    "        events_mod_start,\n",
    "        normal_walking_events,\n",
    "        mod_start_event_id,\n",
    "        normal_walking_event_id,\n",
    "        epoch_tmin,\n",
    "        epoch_tmax,\n",
    "        event_dict\n",
    "    )\n",
    "    print(f\"Total epochs: {len(epochs)}\")\n",
    "    for cls in event_dict.keys():\n",
    "        print(f\"{cls}: {len(epochs[cls])} epochs\", end='; ')\n",
    "    \n",
    "    # events[:, 1] = s # No need to mark the session nr for events again!\n",
    "    epochs.events[:, 1] = s # mark the session nr\n",
    "    \n",
    "    my_annot = mne.Annotations(\n",
    "        onset=events[:, 0]/lfp_sfreq,  # in seconds\n",
    "        duration=len(events)*[epoch_duration],  # in seconds, too\n",
    "        description=events[:, 2],\n",
    "    )\n",
    "    lfp_raw.set_annotations(my_annot)\n",
    "    \n",
    "    fig = lfp_raw.plot(start=0, duration=np.inf, show=False) # lfp_duration\n",
    "    fig.suptitle(f'Session {s}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/session{s}.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    epochs_list.append(epochs)\n",
    "    events_list.append(events)\n",
    "    \n",
    "    print(\"\\n==========================================================\")\n",
    "\n",
    "\n",
    "epochs = mne.concatenate_epochs(epochs_list, verbose=40)\n",
    "events = np.vstack(events_list)\n",
    "events = events[np.argsort(events[:, 0])]  # TODO: Sort by onset time\n",
    "\n",
    "# Preprocessing\n",
    "## Apply band-pass filtering to the raw LFP data.\n",
    "l_freq = 1\n",
    "h_freq = 50\n",
    "epochs.filter(l_freq=l_freq, h_freq=h_freq, fir_design='firwin', verbose=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as plots/event_classes.png\n",
      "Plot saved as plots/epochs.event_classes.png\n"
     ]
    }
   ],
   "source": [
    "Visualise.plot_event_occurrence(events=events, \n",
    "                                epoch_sample_length=epoch_sample_length, \n",
    "                                lfp_sfreq=lfp_sfreq, \n",
    "                                event_dict=event_dict,\n",
    "                                # gait_modulation_event_id=mod_start_event_id, \n",
    "                                # normal_walking_event_id=normal_walking_event_id, \n",
    "                                n_sessions=n_sessions,\n",
    "                                show_fig=False, \n",
    "                                save_fig=True, \n",
    "                                file_name=f'plots/event_classes.png')\n",
    "\n",
    "\n",
    "Visualise.plot_event_occurrence(events=epochs.events, \n",
    "                                epoch_sample_length=epoch_sample_length, \n",
    "                                lfp_sfreq=lfp_sfreq, \n",
    "                                event_dict=event_dict,\n",
    "                                # gait_modulation_event_id=mod_start_event_id, \n",
    "                                # normal_walking_event_id=normal_walking_event_id, \n",
    "                                n_sessions=n_sessions,\n",
    "                                show_fig=False, \n",
    "                                save_fig=True, \n",
    "                                file_name=f'plots/epochs.event_classes.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as plots/event_class_histogram.png\n",
      "Plot saved as plots/epochs.event_class_histogram.png\n"
     ]
    }
   ],
   "source": [
    "Visualise.plot_event_class_histogram(events=events,\n",
    "                                    event_dict=epochs.event_id,\n",
    "                                    n_sessions=n_sessions,\n",
    "                                    show_fig=False, \n",
    "                                    save_fig=True,\n",
    "                                    file_name=f'plots/event_class_histogram.png')\n",
    "\n",
    "Visualise.plot_event_class_histogram(events=epochs.events,\n",
    "                                    event_dict=epochs.event_id,\n",
    "                                    n_sessions=n_sessions,\n",
    "                                    show_fig=False, \n",
    "                                    save_fig=True,\n",
    "                                    file_name=f'plots/epochs.event_class_histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_all_epochs_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Step 1: Train-test split\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_all_epochs_combined\u001b[49m\u001b[38;5;241m.\u001b[39mget_data(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test_all_epochs_combined\u001b[38;5;241m.\u001b[39mget_data(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train_all_epochs_combined\u001b[38;5;241m.\u001b[39mevents[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_all_epochs_combined' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Step 1: Train-test split\n",
    "X_train = train_all_epochs_combined.get_data(copy=True)\n",
    "X_test = test_all_epochs_combined.get_data(copy=True)\n",
    "\n",
    "y_train = train_all_epochs_combined.events[:, -1]\n",
    "y_test = test_all_epochs_combined.events[:, -1]\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')# (n_epochs, n_channels, n_samples_per_epoch)\n",
    "print(f'y_train shape: {y_train.shape}')  # (n_epochs,)\n",
    "print(f'--- Total epochs: {len(y_train)}, with {sum(y_train == -1)} normal walking and {sum(y_train == 1)} event-related gait modulation')\n",
    "\n",
    "print(f'X_test shape: {X_test.shape}')# (n_epochs, n_channels, n_samples_per_epoch)\n",
    "print(f'y_test shape: {y_test.shape}')  # (n_epochs,)\n",
    "print(f'--- Total epochs: {len(y_train)}, with {sum(y_test == -1)} normal walking and {sum(y_test == 1)} event-related gait modulation')\n",
    "\n",
    "# Step 2: Flatten the X array (n_epochs, n_channels, n_samples_per_epoch) -> (n_epochs, n_channels * n_samples_per_epoch)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "\n",
    "\n",
    "# Step 3: Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train a Logistic Regression model\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Output the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw psd\n",
    "raw_spectrum = raw_data_clean.compute_psd(method='welch', fmin=1, fmax=50, n_fft=2048)\n",
    "\n",
    "psd_arr = raw_spectrum.get_data()\n",
    "psd_freqs = raw_spectrum.freqs\n",
    "print(raw_data_clean.get_data().shape)\n",
    "print(f\"PSD data has shape: {psd_arr.shape}  # channels x frequencies\")\n",
    "print(f\"Frequencies has shape: {psd_freqs.shape}  # frequencies\")\n",
    "\n",
    "raw_spectrum.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_spectrum.get_data().shape, raw_data_clean.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs psd: Train set\n",
    "train_epoch_spectrum = train_all_epochs_combined.compute_psd(method='welch', fmax=50)\n",
    "\n",
    "psd_arr = train_epoch_spectrum.get_data()\n",
    "psd_freqs = train_epoch_spectrum.freqs\n",
    "\n",
    "print(train_all_epochs_combined.get_data().shape)\n",
    "print(f\"PSD data has shape: {psd_arr.shape}\")\n",
    "print(f\"Frequencies has shape: {psd_freqs.shape}\")\n",
    "\n",
    "train_epoch_spectrum.plot(average=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch_spectrum['mod_start'].plot(average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch_spectrum['normal_walking'].plot(average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs psd: Test set\n",
    "test_epoch_spectrum = test_all_epochs_combined.compute_psd(method='welch', fmax=50)\n",
    "\n",
    "psd_arr = test_epoch_spectrum.get_data()\n",
    "psd_freqs = test_epoch_spectrum.freqs\n",
    "\n",
    "print(test_all_epochs_combined.get_data().shape)\n",
    "print(f\"PSD data has shape: {psd_arr.shape}\")\n",
    "print(f\"Frequencies has shape: {psd_freqs.shape}\")\n",
    "\n",
    "test_epoch_spectrum.plot(average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch_spectrum['mod_start'].plot(average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch_spectrum['normal_walking'].plot(average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_epochs_combined.get_data().shape, train_epoch_spectrum.get_data().shape\n",
    "\n",
    "\n",
    "\n",
    "# train_all_epochs_combined.events[:, -1].shape, test_all_epochs_combined.events[:, -1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression based on PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Step 1: Train-test split\n",
    "X_train = train_epoch_spectrum.get_data()\n",
    "X_test = test_epoch_spectrum.get_data()\n",
    "\n",
    "y_train = train_all_epochs_combined.events[:, -1]\n",
    "y_test = test_all_epochs_combined.events[:, -1]\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')# (n_epochs, n_channels, n_samples_per_epoch)\n",
    "print(f'y_train shape: {y_train.shape}')  # (n_epochs,)\n",
    "print(f'--- Total epochs: {len(y_train)}, with {sum(y_train == -1)} normal walking and {sum(y_train == 1)} event-related gait modulation')\n",
    "\n",
    "print(f'X_test shape: {X_test.shape}')# (n_epochs, n_channels, n_samples_per_epoch)\n",
    "print(f'y_test shape: {y_test.shape}')  # (n_epochs,)\n",
    "print(f'--- Total epochs: {len(y_train)}, with {sum(y_test == -1)} normal walking and {sum(y_test == 1)} event-related gait modulation')\n",
    "\n",
    "# Step 2: Flatten the X array (n_epochs, n_channels, n_samples_per_epoch) -> (n_epochs, n_channels * n_samples_per_epoch)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "\n",
    "\n",
    "# Step 3: Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train a Logistic Regression model\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Output the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA raw\n",
    "## CODE GOES HERE\n",
    "ica_95PCA = mne.preprocessing.ICA(n_components=0.95, random_state=0)\n",
    "ica_95PCA.fit(inst=lfp_raw)\n",
    "# ica_95PCA.plot_sources(inst=lfp_raw, title=\"ICA sources (95% variance PCA components)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_ica_excluded = ica.apply(inst=lfp_raw.copy())\n",
    "# raw_ica = ica.apply(inst=lfp_raw.copy())\n",
    "\n",
    "# raw_ica_excluded.plot(scalings='auto', start=12, duration=4, title='clraned sensor signals (without noise)')\n",
    "# raw_ica.plot(scalings='auto', start=12, duration=4, title='clraned sensor signals (without noise)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ICA rpochs\n",
    "# ## CODE GOES HERE\n",
    "# ica_95PCA = mne.preprocessing.ICA(n_components=0.95, random_state=0)\n",
    "# ica_95PCA.fit(inst=epochs_raw)\n",
    "# ica_95PCA.plot_sources(inst=epochs_raw, title=\"ICA sources (95% variance PCA components)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute variance explained by PCA components\n",
    "# explained_variance = ica.pca_explained_variance_ / np.sum(ica.pca_explained_variance_)\n",
    "\n",
    "# print(f\"Variance explained by PCA components: {explained_variance}\")\n",
    "# print(f\"Variance explained by first 4 PCA components: {np.sum(explained_variance[:4]) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ica = mne.preprocessing.ICA(random_state=0)\n",
    "# ica.fit(raw_sensors)\n",
    "\n",
    "# # Remove the first ICA component (the random noise) from the data\n",
    "# raw_cleaned = ica.apply(inst=raw_sensors.copy(), exclude=[1,2])\n",
    "# raw_cleaned.plot(scalings='auto', title='clraned sensor signals (without noise)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_raw[\"mod_start\"].plot_image(combine=\"mean\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.plot_epochs_image(\n",
    "    epochs_raw['mod_start'],\n",
    "    picks=[0, 1, 2, 3, 4, 5],\n",
    "    sigma=0.5,\n",
    "    # combine=\"mean\",\n",
    "    # evoked=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evoked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evoked_0 = epochs_raw['trial_start'].average()\n",
    "evoked_4 = epochs_raw['mod_start'].average()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Field Power (GFP)\n",
    "\n",
    "The GFP is the population standard deviation of the signal across channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0 = evoked_0.plot(gfp=True);\n",
    "fig1 = evoked_1.plot(gfp=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_0.plot(gfp=\"only\");\n",
    "evoked_1.plot(gfp=\"only\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp = evoked_0.data.std(axis=0, ddof=0)\n",
    "\n",
    "# Reproducing the MNE-Python plot style seen above\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evoked_0.times, gfp * 1e6, color=\"lime\")\n",
    "ax.fill_between(evoked_0.times, gfp * 1e6, color=\"lime\", alpha=0.2)\n",
    "ax.set(xlabel=\"Time (s)\", ylabel=\"GFP (µV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp = evoked_1.data.std(axis=0, ddof=0)\n",
    "\n",
    "# Reproducing the MNE-Python plot style seen above\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evoked_1.times, gfp * 1e6, color=\"lime\")\n",
    "ax.fill_between(evoked_1.times, gfp * 1e6, color=\"lime\", alpha=0.2)\n",
    "ax.set(xlabel=\"Time (s)\", ylabel=\"GFP (µV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Time-frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.arange(2, 50, 2) # Frequencies from 2 to 50 Hz\n",
    "n_cycles = freqs / 2 # Number of cycles in Morlet wavelet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.arange(7, 30, 3)\n",
    "power = epochs_raw['mod_start'].compute_tfr(\n",
    "    \"morlet\", \n",
    "    n_cycles=2,\n",
    "    return_itc=False, \n",
    "    freqs=freqs,\n",
    "    decim=3,\n",
    "    average=True\n",
    ")\n",
    "power.plot(title='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.arange(7, 30, 3)\n",
    "power = epochs_raw['min_vel'].compute_tfr(\n",
    "    \"morlet\", \n",
    "    n_cycles=2,\n",
    "    return_itc=False, \n",
    "    freqs=freqs,\n",
    "    decim=3,\n",
    "    average=True\n",
    ")\n",
    "power.plot(title='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_fft = mne.time_frequency.csd_fourier(train_all_epochs_combined, fmin=1, fmax=50)\n",
    "csd_mt = mne.time_frequency.csd_multitaper(train_all_epochs_combined, fmin=1, fmax=50, adaptive=True)\n",
    "frequencies = np.arange(1,51, 1)\n",
    "csd_wav = mne.time_frequency.csd_morlet(train_all_epochs_combined, frequencies, decim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict = {\n",
    "    \"Short-time Fourier transform\": csd_fft,\n",
    "    \"Adaptive multitapers\": csd_mt,\n",
    "    \"Morlet wavelet transform\": csd_wav,\n",
    "}\n",
    "for title, csd in plot_dict.items():\n",
    "    (fig,) = csd.mean().plot()\n",
    "    fig.suptitle(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
