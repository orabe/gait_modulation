{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneGroupOut, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "\n",
    "from gait_modulation import FeatureExtractor2\n",
    "from gait_modulation import LSTMClassifier\n",
    "from gait_modulation.utils.utils import load_pkl, initialize_tf, disable_xla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_data():\n",
    "    \"\"\"Load the preprocessed data from the pickles.\"\"\"\n",
    "    patient_epochs_path = os.path.join(\"results\", \"pickles\", \"patients_epochs.pickle\")\n",
    "    subjects_event_idx_dict_path = os.path.join(\"results\", \"pickles\", \"subjects_event_idx_dict.pickle\")\n",
    "\n",
    "    patient_epochs = load_pkl(patient_epochs_path)\n",
    "    subjects_event_idx_dict = load_pkl(subjects_event_idx_dict_path)\n",
    "    \n",
    "    patient_names = np.array(list(patient_epochs.keys()))\n",
    "    print(f\"Loaded data for {len(patient_names)} patients.\")\n",
    "    return patient_epochs, subjects_event_idx_dict, patient_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def preprocess_data(patient_epochs, patient_names, sfreq, feature_handling=\"flatten_chs\", mask_vals=(0.0, 2), features_config=None, n_windows_threshold=None):\n",
    "\n",
    "    feature_extractor = FeatureExtractor2(sfreq, features_config)\n",
    "\n",
    "    # X_grouped is a list where each element is (n_windows_per_trial, n_features)\n",
    "    X_grouped, y_grouped, groups = [], [], []\n",
    "    excluded_count = 0\n",
    "\n",
    "    for patient in patient_names:\n",
    "        epochs = patient_epochs[patient]\n",
    "        \n",
    "        # Extract trial indices\n",
    "        trial_indices = epochs.events[:, 1]  # Middle column contains trial index\n",
    "        unique_trials = np.unique(trial_indices)\n",
    "        # print(f\"- Patient {patient} has {len(unique_trials)} trials\")\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X_patient, y_patient = feature_extractor.extract_features_with_labels(epochs, feature_handling)\n",
    "        \n",
    "        # Group windows by trial\n",
    "        for trial in unique_trials:\n",
    "            trial_mask = trial_indices == trial  # Find windows belonging to this trial\n",
    "            n_windows = sum(trial_mask)\n",
    "            \n",
    "            if n_windows_threshold is not None and n_windows > n_windows_threshold:\n",
    "                # print(f\"Trial {trial} has {n_windows} windows, excluding...\")\n",
    "                excluded_count += 1\n",
    "                continue\n",
    "            \n",
    "            X_grouped.append(X_patient[trial_mask])  # Store all windows of this trial\n",
    "            y_grouped.append(y_patient[trial_mask])  # Store labels for this trial\n",
    "            groups.append(patient)  # Keep track of the patient\n",
    "            \n",
    "            # print(f\"Trial {trial} has {n_windows} windows\")\n",
    "    print(\"Number of excluded trials:\", excluded_count)\n",
    "\n",
    "    X_padded = pad_sequences(X_grouped, dtype='float32', padding='post', value=mask_vals[0])\n",
    "    y_padded = pad_sequences(y_grouped, dtype='int32', padding='post', value=mask_vals[1])\n",
    "\n",
    "    print(\"Padded X shape:\", X_padded.shape)\n",
    "    print(\"Padded y shape:\", y_padded.shape)\n",
    "\n",
    "    assert not np.any(np.isnan(X_padded)), \"X_grouped contains NaNs\"\n",
    "    assert not np.any(np.isnan(y_padded)), \"y_grouped contains NaNs\"\n",
    "    assert X_padded.shape[0] == y_padded.shape[0] == len(groups), \"X, y, and groups should have the same number of trials\"\n",
    "    assert X_padded.shape[1] == y_padded.shape[1], \"X and y should have the same number of windows\"\n",
    "    \n",
    "    padded_data_path = os.path.join(\"results\", \"padded_data.npz\")\n",
    "    np.savez(padded_data_path, X_padded=X_padded, y_padded=y_padded)\n",
    "    print(f\"Padded data saved at {padded_data_path}.\")\n",
    "    \n",
    "    return X_padded, y_padded, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def setup_logging():\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_dir = os.path.join(\"logs\", \"lstm\", \"models\", f\"logs_run_{timestamp}\")\n",
    "    log_dir = os.path.join(model_dir, \"logs\")\n",
    "    history_dir = os.path.join(model_dir, 'training_history')\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(history_dir, exist_ok=True)\n",
    "\n",
    "    log_stream = StringIO()\n",
    "    logging.basicConfig(\n",
    "        stream=log_stream,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "    logging.info(\"Logging setup complete. Starting training process.\")\n",
    "    \n",
    "    return model_dir, history_dir, log_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def build_pipeline(input_shape, n_windows, mask_vals):\n",
    "    models = {\n",
    "        'lstm': LSTMClassifier(input_shape=input_shape)\n",
    "    }\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', 'passthrough'),\n",
    "        ('classifier', models['lstm'])\n",
    "    ])\n",
    "\n",
    "    param_grid = [      \n",
    "        {\n",
    "            'classifier__hidden_dims': [[32, 32]],\n",
    "            'classifier__activations': [['tanh', 'relu']],\n",
    "            'classifier__recurrent_activations': [['sigmoid', 'hard_sigmoid']],\n",
    "            'classifier__dropout': [0.2],\n",
    "            'classifier__dense_units': [n_windows],\n",
    "            'classifier__dense_activation': ['sigmoid'],\n",
    "            'classifier__optimizer': ['adam'],\n",
    "            'classifier__lr': [0.001],\n",
    "            'classifier__patience': [10],\n",
    "            'classifier__epochs': [2],\n",
    "            'classifier__batch_size': [128],\n",
    "            'classifier__threshold': [0.5],\n",
    "            'classifier__loss': ['binary_crossentropy'],\n",
    "            'classifier__mask_vals': [mask_vals],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(LSTMClassifier.masked_accuracy_score),\n",
    "        'f1': make_scorer(LSTMClassifier.masked_f1_score),\n",
    "    }\n",
    "\n",
    "    if any(hasattr(model, \"predict_proba\") for model in models.values()):\n",
    "        scoring['roc_auc'] = make_scorer(LSTMClassifier.masked_roc_auc_score,\n",
    "                                        # needs_proba=True,\n",
    "                                        response_method='predict_proba',\n",
    "                                        multi_class='ovr')\n",
    "\n",
    "    return pipeline, param_grid, scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "initialize_tf()\n",
    "\n",
    "patient_epochs, subjects_event_idx_dict, patient_names = load_data()\n",
    "\n",
    "# Slice patients for testing\n",
    "patient_names = patient_names[:5]\n",
    "patient_epochs = {k: patient_epochs[k] for k in patient_names}\n",
    "subjects_event_idx_dict = {k: subjects_event_idx_dict[k] for k in patient_names}\n",
    "\n",
    "sfreq = patient_epochs[patient_names[0]].info['sfreq']\n",
    "feature_handling = \"flatten_chs\"\n",
    "mask_vals = (0.0, 2)\n",
    "\n",
    "config_path = os.path.join(\"configs\", \"features_config.json\")\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        features_config = json.load(f)\n",
    "    print(f\"Loaded features configuration from {config_path}.\")\n",
    "else:\n",
    "    features_config = None\n",
    "    print(f\"No features configuration file found at {config_path}. Using default configuration.\")\n",
    "\n",
    "features_config = None\n",
    "if features_config is None:\n",
    "    features_config = {\n",
    "        'time_features': {\n",
    "            # 'mean': True,\n",
    "            # 'std': True,\n",
    "            # 'median': True,\n",
    "            # 'skew': True,\n",
    "            # 'kurtosis': True,\n",
    "            # 'rms': True\n",
    "                # peak_to_peak = np.ptp(lfp_data, axis=2)\n",
    "        },\n",
    "        'freq_features': {\n",
    "            'psd_raw': True,\n",
    "                # psd_vals = np.abs(np.fft.rfft(lfp_data, axis=2))\n",
    "            # 'psd_band_mean': True, band power!\n",
    "            # 'psd_band_std': True,\n",
    "            # 'spectral_entropy': True\n",
    "        },\n",
    "        # 'wavelet_features': {\n",
    "        #     'energy': False\n",
    "        # },\n",
    "        # 'nonlinear_features': {\n",
    "        #     'sample_entropy': True,\n",
    "        #     'hurst_exponent': False\n",
    "        # }\n",
    "    }\n",
    "\n",
    "X_padded, y_padded, groups = preprocess_data(patient_epochs, patient_names, sfreq, feature_handling, mask_vals, features_config)\n",
    "\n",
    "n_features = X_padded.shape[2]\n",
    "n_windows = X_padded.shape[1]\n",
    "input_shape = (None, n_features)\n",
    "\n",
    "model_dir, history_dir, log_stream = setup_logging()\n",
    "pipeline, param_grid, scoring = build_pipeline(input_shape, n_windows, mask_vals)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "n_splits = logo.get_n_splits(X_padded, y_padded, groups)\n",
    "print(f\"Total fits: {n_splits * len(param_grid)}\")\n",
    "print(f\"Number of splits: {n_splits}, Number of parameters: {len(param_grid)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting Grid Search...\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=logo,\n",
    "    scoring=scoring,\n",
    "    refit='f1' if 'f1' in scoring else 'accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_padded, y_padded, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "logging.info(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "logging.info(f\"Best Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_.named_steps['classifier'].model\n",
    "model_summary_path = os.path.join(model_dir, \"best_model_summary.txt\")\n",
    "with open(model_summary_path, 'w') as f:\n",
    "    best_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "print(best_model.summary())\n",
    "print(f\"Best model summary saved at {model_summary_path}.\")\n",
    "\n",
    "best_model_path = os.path.join(model_dir, \"best_lstm_model.h5\")\n",
    "keras_model_path = os.path.join(model_dir, 'best_lstm_model.keras')\n",
    "best_model.save(best_model_path)\n",
    "save_model(best_model, keras_model_path)\n",
    "print(f\"Best LSTM model saved at {best_model_path}.\")\n",
    "logging.info(f\"Best LSTM model saved at {best_model_path}.\")\n",
    "\n",
    "best_params_path = os.path.join(model_dir, 'best_params.json')\n",
    "cv_results_path = os.path.join(model_dir, 'cv_results.csv')\n",
    "evaluation_metrics_path = os.path.join(model_dir, 'evaluation_metrics.json')\n",
    "\n",
    "for fold, history in enumerate(grid_search.best_estimator_.named_steps['classifier'].history_):\n",
    "    history_path = os.path.join(history_dir, f'training_history_fold_{fold + 1}.json')\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    print(f\"Training history for fold {fold + 1} saved at {history_path}\")\n",
    "\n",
    "with open(best_params_path, 'w') as f:\n",
    "    json.dump(grid_search.best_params_, f)\n",
    "print(f\"Best parameters saved at {best_params_path}\")\n",
    "\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df.to_csv(cv_results_path, index=False)\n",
    "print(f\"Cross-validation results saved at {cv_results_path}\")\n",
    "\n",
    "plot_model(best_model, to_file=os.path.join(model_dir, 'model_architecture.png'), show_shapes=True)\n",
    "print(f\"Model architecture plot saved at {os.path.join(model_dir, 'model_architecture.png')}.\")\n",
    "\n",
    "log_file_path = os.path.join(model_dir, 'training.log')\n",
    "with open(log_file_path, 'w') as f:\n",
    "    f.write(log_stream.getvalue())\n",
    "print(f\"Training logs saved at {log_file_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait_modulation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
